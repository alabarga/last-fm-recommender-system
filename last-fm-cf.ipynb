{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song Recommender System using Last.fm dataset\n",
    "\n",
    "#### Task\n",
    "* Build a song recommender system for female users in the United Kingdom.\n",
    "\n",
    "#### Dataset\n",
    "* [Last.fm](http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/index.html)\n",
    "\n",
    "#### Algorithm\n",
    "* Memory-based Collaborative Filtering (CF) Recommender System\n",
    "\n",
    "There are two common approaches for collaborative filtering (CF) recommender systems, one is memory-based and one is model-based.  \n",
    "\n",
    "In memory-based CF, we first compute item-user matrix in memory, calculate user cosine similarity (for user-based) or item cosine similarity (for item-based), and make prediction based on similarity measures (no learning is needed in this approach). The problems of memory-based CF are sparsity and scalability. To boost computational efficiency, we can leverage on sparse matrix package in scipy. This notebook demonstrates memory-based CF for song recommender system using last.fm dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-based CF from scratch (item-based / user-based)\n",
    "\n",
    "* Step 1: Compute user-item matrix\n",
    "* Step 2: Compute item cosine similarity / user cosine similarity\n",
    "* Step 3: Make item-based / user-based predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359347"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "user_data_filepath = 'data/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv'\n",
    "user_profiles_filepath = 'data/lastfm-dataset-360K/usersha1-profile.tsv'\n",
    "\n",
    "df_data = pd.read_table(user_data_filepath, header = None, \n",
    "                        names = ['user', 'musicbrainz-artist-id', 'artist', 'plays'],\n",
    "                        usecols = ['user', 'artist', 'plays'])\n",
    "\n",
    "df_user = pd.read_table(user_profiles_filepath,\n",
    "                          header = None,\n",
    "                          names = ['user', 'gender', 'age', 'country', 'signup'],\n",
    "                          usecols = ['user', 'gender','country'])\n",
    "\n",
    "len(df_data) #17535655\n",
    "len(df_user) #359347"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only use part of the dataset. First, limit the data set to female users in United Kingdom, then prepare the data into dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288780"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the data set to female users in United Kingdom only\n",
    "df_user_UK = df_user[df_user['country']=='United Kingdom'].drop('country', axis=1)\n",
    "df_user_UK_female = df_user_UK[df_user_UK['gender']=='f'].drop('gender', axis=1)\n",
    "len(df_user_UK_female) #5851\n",
    "\n",
    "# Merge the two dataframes\n",
    "df = df_data.merge(df_user_UK_female, left_on='user', right_on ='user', how='inner')\n",
    "df = df.groupby(['user','artist'], as_index=False).sum()\n",
    "len(df) # 288780"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrative purpose, I only use top 1% artists (this also help remove noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sukilau/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:9: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 artists: \n",
      "                        artist  total_plays\n",
      "252495            the beatles     30466827\n",
      "217884              radiohead     27426234\n",
      "73856                coldplay     16686772\n",
      "212497             pink floyd     15943557\n",
      "186241              metallica     15481852\n",
      "194163                   muse     15451683\n",
      "199674        nine inch nails     14075619\n",
      "220129  red hot chili peppers     13547741\n",
      "171169            linkin park     12836638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "202917"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find total number of plays for each artist\n",
    "df_artist = df_data.groupby(['artist'])['plays'].sum().reset_index().rename(columns = {'plays': 'total_plays'})\n",
    "df_artist.describe() \n",
    "\n",
    "# Find total number of plays of the 99th percentile artist\n",
    "df_artist['total_plays'].quantile(0.99)  #198482.5899999995\n",
    "\n",
    "# Set threshold = 200000 clicks, limit the data set to artists with more than 200000 clicks\n",
    "df_top_artist = df_artist[df_artist['total_plays']>200000].sort('total_plays', ascending=False)\n",
    "print(\"Top 10 artists: \\n\", df_top_artist[0:9])\n",
    "\n",
    "top_artist = list(df_top_artist['artist'])\n",
    "df = df[df['artist'].isin(top_artist)]\n",
    "df.head()\n",
    "len(df) #202917"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute user-item matrix\n",
    "\n",
    "* user-item matrix dim = (num of users, num of items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create item-user matrix, where each row is the artist i (item) and each column is the user j,\n",
    "## and the entry is the number of total plays clicked by user j to artist i.\n",
    "matrix = df.pivot(index ='artist', columns='user', values='plays').fillna(0)\n",
    "matrix_sparse = csr_matrix(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# checking\n",
    "matrix.shape #(30593, 5841)   (2840, 5828)-top1%\n",
    "matrix.index.get_loc('radiohead') #1976\n",
    "matrix.index[1976]  #'radiohead'\n",
    "matrix.iloc[1976]\n",
    "matrix.loc[\"radiohead\"] #same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cosine similarity between users and items \n",
    "\n",
    "* item_similarity matrix dim = (num of items, num of items)\n",
    "* user_similarity matrix dim = (num of users, num of users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_similarity = pairwise_distances(matrix_sparse, metric='cosine')\n",
    "user_similarity = pairwise_distances(matrix_sparse.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2840)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking\n",
    "item_similarity.shape #(2840, 2840)\n",
    "user_similarity.shape #(5828, 5828)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction\n",
    "\n",
    "* item_prediction matrix dim = (num of users, num of items)\n",
    "* user_prediction matrix dim = (num of users, num of items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "def predict(matrix, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = matrix.mean(axis=1)\n",
    "        ratings_diff = (matrix - mean_user_rating)\n",
    "        pred = mean_user_rating + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif type == 'item':\n",
    "        pred = matrix.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred\n",
    "\n",
    "item_prediction = predict(matrix_sparse.T, item_similarity, type='item')\n",
    "user_prediction = predict(matrix_sparse.T, user_similarity, type='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-based CF using KNNBasic in Surprise\n",
    "\n",
    "We can also use [Surprise](http://surprise.readthedocs.io/en/stable/getting_started.html) (Recommender System library) to implement KNN-inspired algorithms for memory-based CF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "user: 000cdac4253f2dcecbd3dff8c5d7cf8cf0426c7a item: john mayer r_ui = None   est = 128.03   {'actual_k': 5, 'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD, KNNBasic, evaluate\n",
    "\n",
    "data = Dataset.load_from_df(df[['user', 'artist', 'plays']], Reader(rating_scale=(1, df['plays'].max())))\n",
    "\n",
    "# First, train the algortihm to compute the similarities between items\n",
    "# training is very slow..\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# compute cosine similarities between items\n",
    "sim_options = {'name': 'cosine','user_based': False}\n",
    "knn = KNNBasic(k=5, sim_options=sim_options)\n",
    "knn.train(trainset)\n",
    "\n",
    "# predict a certain item\n",
    "userid = '000cdac4253f2dcecbd3dff8c5d7cf8cf0426c7a'\n",
    "itemid = 'john mayer'\n",
    "print(knn.predict(userid, itemid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual no. of plays of john mayer by user 0 = 197.0\n"
     ]
    }
   ],
   "source": [
    "# actual rating \n",
    "print(\"Actual no. of plays of john mayer by user 0 =\", matrix.loc[\"john mayer\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
